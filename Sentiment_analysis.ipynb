{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привет, меня зовут Артем Хуршудов. Сегодня я проверю твой проект.\n",
    "<br> Дальнейшее общение будет происходить на \"ты\" если это не вызывает никаких проблем.\n",
    "<br> Желательно реагировать на каждый мой комментарий ('исправил', 'не понятно как исправить ошибку', ...)\n",
    "<br> Пожалуйста, не удаляй комментарии ревьюера, так как они повышают качество повторного ревью.\n",
    "\n",
    "Комментарии будут в <font color='green'>зеленой</font>, <font color='blue'>синей</font> или <font color='red'>красной</font> рамках:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Если все сделано отлично\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Если можно немного улучшить\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка:</b> Если требуются исправления. Работа не может быть принята с красными комментариями.\n",
    "</div>\n",
    "\n",
    "-------------------\n",
    "\n",
    "Будет очень хорошо, если ты будешь помечать свои действия следующим образом:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий студента:</b> ...\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Были внесены следующие изменения ...\n",
    "</div>\n",
    "\n",
    "<font color='green'><b>Полезные (и просто интересные) материалы:</b></font> \\\n",
    "Для работы с текстами используют и другие подходы. Например, сейчас активно используются RNN (LSTM) и трансформеры (BERT и другие с улицы Сезам, например, ELMO). НО! Они не являются панацеей, не всегда они нужны, так как и TF-IDF или Word2Vec + модели из классического ML тоже могут справляться. \\\n",
    "BERT тяжелый, существует много его вариаций для разных задач, есть готовые модели, есть надстройки над библиотекой transformers. Если, обучать BERT на GPU (можно в Google Colab или Kaggle), то должно быть побыстрее.\\\n",
    "https://huggingface.co/transformers/model_doc/bert.html \\\n",
    "https://t.me/renat_alimbekov \\\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/ - Про LSTM \\\n",
    "https://web.stanford.edu/~jurafsky/slp3/10.pdf - про энкодер-декодер модели, этеншены\\\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html - официальный гайд\n",
    "по трансформеру от создателей pytorch\\\n",
    "https://transformer.huggingface.co/ - поболтать с трансформером \\\n",
    "Библиотеки: allennlp, fairseq, transformers, tensorflow-text — множествореализованных\n",
    "методов для трансформеров методов NLP \\\n",
    "Word2Vec https://radimrehurek.com/gensim/models/word2vec.html \n",
    "\n",
    "<font color='green'>Пример BERT с GPU:\n",
    "```python\n",
    "%%time\n",
    "from tqdm import notebook\n",
    "batch_size = 2 # для примера возьмем такой батч, где будет всего две строки датасета\n",
    "embeddings = [] \n",
    "for i in notebook.tqdm(range(input_ids.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(input_ids[batch_size*i:batch_size*(i+1)]).cuda() # закидываем тензор на GPU\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.cuda()\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy()) # перевод обратно на проц, чтобы в нумпай кинуть\n",
    "        del batch\n",
    "        del attention_mask_batch\n",
    "        del batch_embeddings\n",
    "        \n",
    "features = np.concatenate(embeddings) \n",
    "```\n",
    "Можно сделать предварительную проверку на наличие GPU.\\\n",
    "Например, так: ```device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")```\\\n",
    "Тогда вместо .cuda() нужно писать .to(device)\n",
    "\n",
    "Если понравилась работа с текстами, то можешь посмотреть очень интересный (но очень-очень сложный) курс лекций: https://github.com/yandexdataschool/nlp_course .\n",
    "</font>\n",
    "\n",
    "### <font color='orange'>Общее впечатление</font>\n",
    "* Большое спасибо за проделанную работу. Видно, что приложено много усилий.\n",
    "* Радует, что ноутбук хорошо структурирован. Приятно проверять такие работы.\n",
    "* Отлично, что стоп-слова были исключены при векторизации!\n",
    "* Над этим проектом нужно будет еще немного поработать. Однако, изменения не должны занять много времени.\n",
    "* С радостью отвечу на твои вопросы, если они есть. Лучше всего их собрать в следующей ячейке. Жду новую версию проекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привет, большое спасибо за ревью! \n",
    "\n",
    "Подскажи, пожалуйста, как правильно поступать с данными, если количество строк в датафрейме не кратно, размеру батча (в таком случае последний батч не генерируется, и количество признаков расходится с кол-вом целевыхпризнаков). В этом проекте я использовал размерность кратную размеру батча."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большое спасибо за комментарий и ссылки, добавил в проект раздел с `BERT`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>Общее впечатление (ревью 2)</font>\n",
    "* Для удобства все новые комментарии обозначены фразой \"ревью 2\".\n",
    "* Удачи в доработке!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет (ревью 2): </b> Обычно, размер батча достаточно маленький и эти данные просто удаляют. В этом проекте у нас же вообще не используются батчи (только в БЕРТе).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий студента:</b> Понял, спасибо!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'>Общее впечатление (ревью 2)</font>\n",
    "* После исправлений проект улучшился и теперь он может быть зачтен.\n",
    "* Удачи в дальнейшем обучении и следующих работах!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Общее-впечатление\" data-toc-modified-id=\"Общее-впечатление-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление</font></a></span></li><li><span><a href=\"#Общее-впечатление-(ревью-2)\" data-toc-modified-id=\"Общее-впечатление-(ревью-2)-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление (ревью 2)</font></a></span></li><li><span><a href=\"#Общее-впечатление-(ревью-2)\" data-toc-modified-id=\"Общее-впечатление-(ревью-2)-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление (ревью 2)</font></a></span></li></ul></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>BERT</a></span></li></ul></li><li><span><a href=\"#Тестирование\" data-toc-modified-id=\"Тестирование-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Тестирование</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\artem\\anaconda3\\envs\\practicum\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\artem\\anaconda3\\envs\\practicum\\lib\\site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\artem\\anaconda3\\envs\\practicum\\lib\\site-packages (from lightgbm) (1.20.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\artem\\anaconda3\\envs\\practicum\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\artem\\anaconda3\\envs\\practicum\\lib\\site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\artem\\anaconda3\\envs\\practicum\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\artem\\anaconda3\\envs\\practicum\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import nltk\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Отлично, что все импорты собраны в первой ячейке ноутбука! Если у того, кто будет запускать твой ноутбук будут отсутствовать некоторые библиотеки, то он это увидит сразу, а не в процессе!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141152</th>\n",
       "      <td>Your vandalism\\nPlease stop your stupidity. No...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67937</th>\n",
       "      <td>\"\\n\\nIs it necessary to have this many picture...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53259</th>\n",
       "      <td>look dude, they came, they saw, they kicked cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95824</th>\n",
       "      <td>Fullerton Securities \\n\\n Company Overview \\n\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116112</th>\n",
       "      <td>\"\\n\\n Supposed influence on historical events ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133635</th>\n",
       "      <td>I removed his bullshits. 114.179.18.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73608</th>\n",
       "      <td>REDIRECT Talk:German Americans/Archive 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155669</th>\n",
       "      <td>As spoken in.... (don't mention the States!) \\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79960</th>\n",
       "      <td>\"\\n\\nI will think about what you said.  I remo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118302</th>\n",
       "      <td>Great page\\nLove that phrase.  Hope this artic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138421</th>\n",
       "      <td>Tiger Woods likes Seb az86556's unicorns</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113926</th>\n",
       "      <td>\"\"\"muscle\"\" and block me. This shit is getting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44170</th>\n",
       "      <td>algo = old(7d)\\n|archive = User talk:Theblades...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74890</th>\n",
       "      <td>Khanum \\n\\nSo... How about a interwiki link fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23442</th>\n",
       "      <td>\"\\n\\n\"\"Piping\"\" is when you supply alternative...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58432</th>\n",
       "      <td>\":Tut tut. I thought I said, \"\"Goodbyeee!\"\" an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556</th>\n",
       "      <td>I always start off by assuming good faith, but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9834</th>\n",
       "      <td>\"\\n\\n Comments \\n\\nPer \"\"Note that in many cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16919</th>\n",
       "      <td>\"\\n\\nSpeedy deletion of ArtofRhyme.com\\n A tag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57348</th>\n",
       "      <td>\"So I definitely move the page to List of nota...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105277</th>\n",
       "      <td>You've made me so angry that I've decided to t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151253</th>\n",
       "      <td>\"\\n\\nUser Category for Discussion\\nA category ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25106</th>\n",
       "      <td>Casting rumors and speculation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120945</th>\n",
       "      <td>\"\\n\\n This is your last warning. You will be b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116693</th>\n",
       "      <td>\"\\nAugust 2007\\n Dear 214.13.212.180, hello, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29936</th>\n",
       "      <td>) 18:10, 24 March 2014 (UTC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76707</th>\n",
       "      <td>\"\\n\\nThis page has been deleted twice before a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33141</th>\n",
       "      <td>\"\\n\\n Art Gallery last 2 days in wiki \\n\\nHere...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155617</th>\n",
       "      <td>\"\\n Well, like I said, those are what the sour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116306</th>\n",
       "      <td>I'M GETTING ANGRY AT YOU!!!!!!!!!\\nYOU ARE GET...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131717</th>\n",
       "      <td>\"\\nOrphaned non-free media (Image:ParamountPar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43609</th>\n",
       "      <td>I'm trying to work on that.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55475</th>\n",
       "      <td>\"\\n Technically all articles should remain.  E...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111856</th>\n",
       "      <td>You mean this video from youtube: http://www.y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109077</th>\n",
       "      <td>AnonMoos \\n\\n''''DO NOT LISTEN TO ANONMOOS''''</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31930</th>\n",
       "      <td>Thanks for your note on my talk - I also poste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90911</th>\n",
       "      <td>Support.  The status quo was stable, and is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145456</th>\n",
       "      <td>\"\\nYou're the one who's making an \"\"edit war\"\"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93651</th>\n",
       "      <td>Hi yourself \\n\\nYou're not missing anything, i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125330</th>\n",
       "      <td>These are not spam links!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59512</th>\n",
       "      <td>50.25.13.13 \\n\\n, who you blocked, is back edi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26194</th>\n",
       "      <td>Ignoring the accusations of bias, the two dele...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26213</th>\n",
       "      <td>New Picture is released and approved by @DanWa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64498</th>\n",
       "      <td>FYI: free, full-access, 1-year HighBeam Resear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Anytime you want a nightcap, tell me.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26743</th>\n",
       "      <td>Archivesyou have a message re your last change...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77680</th>\n",
       "      <td>Is the fact that all international art organis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143804</th>\n",
       "      <td>Somebody named Werieth removed it from the Pih...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116008</th>\n",
       "      <td>REDIRECT Talk:Fremont (Amtrak station)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97078</th>\n",
       "      <td>Merger proposal with Lance Armstrong doping al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "141152  Your vandalism\\nPlease stop your stupidity. No...      1\n",
       "67937   \"\\n\\nIs it necessary to have this many picture...      0\n",
       "53259   look dude, they came, they saw, they kicked cr...      1\n",
       "95824   Fullerton Securities \\n\\n Company Overview \\n\\...      0\n",
       "116112  \"\\n\\n Supposed influence on historical events ...      0\n",
       "133635             I removed his bullshits. 114.179.18.37      1\n",
       "73608            REDIRECT Talk:German Americans/Archive 2      0\n",
       "155669  As spoken in.... (don't mention the States!) \\...      0\n",
       "79960   \"\\n\\nI will think about what you said.  I remo...      0\n",
       "118302  Great page\\nLove that phrase.  Hope this artic...      0\n",
       "138421           Tiger Woods likes Seb az86556's unicorns      0\n",
       "113926  \"\"\"muscle\"\" and block me. This shit is getting...      1\n",
       "44170   algo = old(7d)\\n|archive = User talk:Theblades...      0\n",
       "74890   Khanum \\n\\nSo... How about a interwiki link fo...      0\n",
       "23442   \"\\n\\n\"\"Piping\"\" is when you supply alternative...      0\n",
       "58432   \":Tut tut. I thought I said, \"\"Goodbyeee!\"\" an...      1\n",
       "15556   I always start off by assuming good faith, but...      0\n",
       "9834    \"\\n\\n Comments \\n\\nPer \"\"Note that in many cas...      0\n",
       "16919   \"\\n\\nSpeedy deletion of ArtofRhyme.com\\n A tag...      0\n",
       "57348   \"So I definitely move the page to List of nota...      0\n",
       "105277  You've made me so angry that I've decided to t...      0\n",
       "151253  \"\\n\\nUser Category for Discussion\\nA category ...      0\n",
       "25106                      Casting rumors and speculation      0\n",
       "120945  \"\\n\\n This is your last warning. You will be b...      0\n",
       "116693  \"\\nAugust 2007\\n Dear 214.13.212.180, hello, a...      0\n",
       "29936                         ) 18:10, 24 March 2014 (UTC      0\n",
       "76707   \"\\n\\nThis page has been deleted twice before a...      0\n",
       "33141   \"\\n\\n Art Gallery last 2 days in wiki \\n\\nHere...      0\n",
       "155617  \"\\n Well, like I said, those are what the sour...      1\n",
       "116306  I'M GETTING ANGRY AT YOU!!!!!!!!!\\nYOU ARE GET...      1\n",
       "131717  \"\\nOrphaned non-free media (Image:ParamountPar...      0\n",
       "43609                         I'm trying to work on that.      0\n",
       "55475   \"\\n Technically all articles should remain.  E...      0\n",
       "111856  You mean this video from youtube: http://www.y...      0\n",
       "109077     AnonMoos \\n\\n''''DO NOT LISTEN TO ANONMOOS''''      0\n",
       "31930   Thanks for your note on my talk - I also poste...      0\n",
       "90911   Support.  The status quo was stable, and is a ...      0\n",
       "145456  \"\\nYou're the one who's making an \"\"edit war\"\"...      0\n",
       "93651   Hi yourself \\n\\nYou're not missing anything, i...      0\n",
       "125330                          These are not spam links!      0\n",
       "59512   50.25.13.13 \\n\\n, who you blocked, is back edi...      0\n",
       "26194   Ignoring the accusations of bias, the two dele...      0\n",
       "26213   New Picture is released and approved by @DanWa...      0\n",
       "64498   FYI: free, full-access, 1-year HighBeam Resear...      0\n",
       "504                 Anytime you want a nightcap, tell me.      0\n",
       "26743   Archivesyou have a message re your last change...      1\n",
       "77680   Is the fact that all international art organis...      0\n",
       "143804  Somebody named Werieth removed it from the Pih...      0\n",
       "116008             REDIRECT Talk:Fremont (Amtrak station)      0\n",
       "97078   Merger proposal with Lance Armstrong doping al...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Данные загружены корреткно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
    "data['text'] = data['text'].str.lower()\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Очистка была сделана верно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим наличие дубликатов и удалим их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1758"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка (ревью 2):</b> В прошлой итерации не было даунсемплинга.\n",
    "    \n",
    "Изменять баланс классов можно только в треине. В тесте и в валидации он должен быть такой же, как и в исходной выборке. Иначе, результаты получаются некорректными.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Спасибо ,изменил даунсемплинг на апсемплинг, только для тренировочной выборке\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех (ревью 3):</b> ОК.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизируем тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "data['lem_text'] = data['text'].apply(lambda x: \" \".join([lemma.lemmatize(i) for i in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка:</b> Этот лемматизатор нужно применять к словам, а не ко всему тексту сразу.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Спасибо, лемматизировал каждое слово.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех (ревью 2):</b> Верно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разделим выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(data,test_size=0.4,random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим баланс классов в тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    85028\n",
      "1     9643\n",
      "Name: toxic, dtype: int64\n",
      "отрицательный класс встречается в 8.82 раз чаще\n"
     ]
    }
   ],
   "source": [
    "print(train['toxic'].value_counts())\n",
    "print('отрицательный класс встречается в {} раз чаще'.format(round(train['toxic'].value_counts()[0]/train['toxic'].value_counts()[1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = pd.DataFrame()\n",
    "upsampled['lem_text'],upsampled['toxic'] = upsample(train['lem_text'],train['toxic'],9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    86787\n",
       "0    85028\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = count_tf_idf.fit_transform(upsampled['lem_text'])\n",
    "features_test = count_tf_idf.transform(test['lem_text'])\n",
    "target_train = upsampled['toxic']\n",
    "target_test = test['toxic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_valid,features_test,target_valid,target_test = train_test_split(features_test,target_test,train_size=0.5,random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Разбиение было сделано верно. Отлично, что векторизатор был обучен только на тренировочной части данных.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6364309514994445\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_jobs=8,random_state=12345)\n",
    "model.fit(features_train,target_train)\n",
    "predictions = model.predict(features_valid)\n",
    "print(f1_score(target_valid,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.7354497354497354\n",
      "{'penalty': 'l2', 'solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=500,n_jobs=-1,random_state=12345)\n",
    "parametrs  = {'solver':['sag','saga'],\n",
    "            'penalty':['l2']}\n",
    "grid = GridSearchCV(model,parametrs,scoring='f1',cv=2)\n",
    "grid.fit(features_train,target_train)\n",
    "predictions = grid.predict(features_valid)\n",
    "log_reg_estimator = grid.best_estimator_\n",
    "print('F1 - ',f1_score(target_valid,predictions))\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Напомню, что внутри кросс-валидации происходит разбиение выборки на треин и валидацию. Однако, в таком случае векторизатор обучен на всей выборке, а это не совсем корректно. Для избежания такого эффекта можно использовать <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">пайплайн</a>. <a href=\"https://medium.com/analytics-vidhya/ml-pipelines-using-scikit-learn-and-gridsearchcv-fe605a7f9e05\">Тут</a> есть пример.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Большое спасибо!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.7692804130364633\n",
      "порог -  0.68\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for i in np.arange(0.10,0.7,0.01):\n",
    "    model = log_reg_estimator\n",
    "    probabilities = grid.predict_proba(features_valid)\n",
    "    probabilities_one_valid = probabilities[:, 1]\n",
    "    predictions = probabilities_one_valid > i\n",
    "    score = f1_score(target_valid,predictions)\n",
    "    if score > best_score: \n",
    "        best_score = score\n",
    "        best_threshold = i\n",
    "print('F1 - ',best_score)\n",
    "print('порог - ',best_threshold.round(2))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 - 0.7550960978450786\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier(n_estimators=500,random_state=12345,n_jobs=8)\n",
    "model.fit(features_train,target_train)\n",
    "predictions = model.predict(features_valid)\n",
    "print('F1 -',f1_score(target_valid,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Молодец, что попробовал разные модели в этом шаге!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "токенизируем тексты для `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(16000).reset_index(drop=True) # ограничим выборку 16 000 значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "tokenized = data['text'].apply(\n",
    "  lambda x: tokenizer.encode(x,truncation=True,max_length=512,add_special_tokens=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "config = transformers.DistilBertConfig()\n",
    "model = transformers.DistilBertModel(config)\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ad5572eb9f45a3a304578bb5d06c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    # преобразуем данные\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "    # преобразуем маску\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "    batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "    # преобразуем элементы методом numpy() к типу numpy.array\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].cpu().data.numpy()) \n",
    "features_bert = np.concatenate(embeddings)\n",
    "features_bert = pd.DataFrame(features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_bert,features_valid_bert,target_train_bert,target_valid_bert = train_test_split(features_bert,data['toxic'],test_size=0.2)\n",
    "features_valid_bert,features_test_bert,target_valid_bert,target_test_bert = train_test_split(features_valid_bert,target_valid_bert,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11496\n",
      "1     1304\n",
      "Name: toxic, dtype: int64\n",
      "отрицательный класс встречается в 8.82 раз чаще\n"
     ]
    }
   ],
   "source": [
    "print(target_train_bert.value_counts())\n",
    "print('отрицательный класс встречается в {} раз чаще'.format(round(target_train_bert.value_counts()[0]/target_train_bert.value_counts()[1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_features_train = pd.DataFrame()\n",
    "up_target_train = pd.DataFrame()\n",
    "\n",
    "up_features_train,up_target_train = upsample(features_train_bert,target_train_bert,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.4475524475524475\n",
      "{'C': 0.8, 'penalty': 'l2', 'solver': 'sag'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem\\anaconda3\\envs\\practicum\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=500,random_state=12345)\n",
    "parametrs  = {'solver':['sag'],\n",
    "            'penalty':['l2'],\n",
    "            'C':np.arange(0.6,1,0.2)}\n",
    "grid = GridSearchCV(model,parametrs,scoring='f1',cv=2,n_jobs=-1)\n",
    "grid.fit(up_features_train,up_target_train)\n",
    "predictions = grid.predict(features_valid_bert)\n",
    "print('F1 - ',f1_score(target_valid_bert,predictions))\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.5437352245862884\n",
      "порог -  0.69\n"
     ]
    }
   ],
   "source": [
    "model = grid\n",
    "best_score = 0\n",
    "for i in np.arange(0.10,0.7,0.01):\n",
    "    probabilities = model.predict_proba(features_valid_bert)\n",
    "    probabilities_one_valid = probabilities[:, 1]\n",
    "    predictions = probabilities_one_valid > i\n",
    "    score = f1_score(target_valid_bert,predictions)\n",
    "    if score > best_score: \n",
    "        best_score = score\n",
    "        best_threshold = i\n",
    "print('F1 - ',best_score)\n",
    "print('порог - ',best_threshold.round(2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим лучшую модель (логистическую регрессию) на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 -  0.7618276085547635\n"
     ]
    }
   ],
   "source": [
    "model = log_reg_estimator\n",
    "probabilities = model.predict_proba(features_test)\n",
    "probabilities_one_valid = probabilities[:, 1]\n",
    "predictions = probabilities_one_valid > 0.68\n",
    "print('F1 - ',f1_score(target_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Тестирование было сделано верно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе проекта мы:\n",
    "1. Предобработали данные (удалили лишние символы, лемматизировали тексты,рассчитали TF-IDF,сбалансировали целевой признак,разбили данные на три выборки)\n",
    "2. Обучили модели машинного обучения и проверили их на валидационной выборке.\n",
    "3. Проверирили лучшую модель на тестовой выборке (метрика F1 - 0.88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Приятно видеть вывод в конце проекта!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "6d04722112be327fab217074d2c198fb2e5e83ba2928a8d6adaa4e56a2db9e0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
